# -*- coding: utf-8 -*-
"""COVID-COMORBIDITIES.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13_DLi-pOndIRuIX7YxiWdfQbBKWTbvy2
"""

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Load the dataset
data = pd.read_csv('combined_covid_comorbidities.csv')

# Impute missing values in RMST using the mean
data['RMST'].fillna(data['RMST'].mean(), inplace=True)

# Feature Engineering - Adding a feature for the range of confidence intervals
data['CI Range'] = data['CI Upper'] - data['CI Lower']

# Preparing the dataset for model training
X = data.drop(['RMST', 'Time Interval Start'], axis=1)
y = data['RMST']

# Define imputers for numerical and categorical data
numerical_imputer = SimpleImputer(strategy='mean')  # Imputes NaNs using the mean of the column
categorical_imputer = SimpleImputer(strategy='most_frequent')  # Imputes NaNs using the mode of the column

# Encode categorical data and scale numerical data
numerical_features = X.select_dtypes(include=['float64', 'int64']).columns
categorical_features = X.select_dtypes(include=['object']).columns

preprocessor = ColumnTransformer(
    transformers=[
        ('num', Pipeline([('imputer', numerical_imputer), ('scaler', StandardScaler())]), numerical_features),
        ('cat', Pipeline([('imputer', categorical_imputer), ('encoder', OneHotEncoder())]), categorical_features)
    ])

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Pipeline for training RandomForestRegressor
regressor_model = Pipeline(steps=[('preprocessor', preprocessor),
                                  ('regressor', RandomForestRegressor())])
regressor_model.fit(X_train, y_train)
print('Random Forest Regressor model score:', regressor_model.score(X_test, y_test))

# Building a Vanilla DNN
def build_dnn(input_shape):
    model = Sequential([
        Dense(128, activation='relu', input_shape=(input_shape,)),
        Dense(64, activation='relu'),
        Dense(1)  # Output layer for regression
    ])
    model.compile(optimizer='adam', loss='mse')
    return model

# Initialize and train DNN
pipeline = Pipeline(steps=[('preprocessor', preprocessor)])
X_train_prepared = pipeline.fit_transform(X_train)
X_test_prepared = pipeline.transform(X_test)
dnn_model = build_dnn(X_train_prepared.shape[1])
dnn_model.fit(X_train_prepared, y_train, epochs=50, batch_size=32)

# Evaluate DNN model
mse = dnn_model.evaluate(X_test_prepared, y_test)
print('DNN model MSE:', mse)

from sklearn.linear_model import LinearRegression

# After training the RandomForest and DNN models
# Assume RandomForest predictions are rf_predictions and DNN predictions are dnn_predictions

# Assuming you have set up the regressor_model pipeline correctly
# Fit the regressor model with training data
regressor_model.fit(X_train, y_train)

# Predict using the same pipeline (this will ensure preprocessing steps are applied consistently)
rf_predictions = regressor_model.predict(X_test)  # Note: Use X_test, not X_test_prepared

# For DNN, since it requires scaled features
# Ensure DNN uses the same preprocessing pipeline
pipeline.fit(X_train, y_train)  # Fit preprocessing and DNN model
dnn_predictions = dnn_model.predict(pipeline.transform(X_test)).flatten()  # Ensure preprocessing and then predict

# Combine and stack predictions, etc., as previously described


# Combine predictions to form new features for the meta-model
stacked_features = np.column_stack((rf_predictions, dnn_predictions))

# Train a meta-model (e.g., Linear Regression) on these combined predictions
meta_model = LinearRegression()
meta_model.fit(stacked_features, y_test)  # Using y_test to fit is typically not done; you should split your data differently for a true stacking setup

# Predict using the meta-model
final_predictions = meta_model.predict(stacked_features)

# Evaluate the final model (meta-model)
final_mse = mean_squared_error(y_test, final_predictions)
print("Stacking Ensemble MSE:", final_mse)